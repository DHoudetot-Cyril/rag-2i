# RAG-2i : Assistant Documentaire Intelligent

**RAG-2i** est un syst√®me de **Retrieval-Augmented Generation (RAG)** con√ßu pour interroger une base de connaissances documentaire interne en toute confidentialit√©. Il combine la puissance des LLMs locaux avec une base de donn√©es vectorielle pour fournir des r√©ponses factuelles et sourc√©es.

## üöÄ Fonctionnalit√©s
- **Chat en langage naturel** : Posez vos questions en fran√ßais.
- **Sources cit√©es** : Chaque r√©ponse indique pr√©cis√©ment les documents utilis√©s.
- **Architecture Locale** : Vos donn√©es ne sortent pas de votre infrastructure (sauf si configur√© autrement).
- **Multi-formats** : Support des fichiers PDF, DOCX, PPTX.

## üõ† Stack Technique
- **Frontend** : React + Vite
- **Backend API** : FastAPI (Python)
- **Base Vectorielle** : Qdrant (Docker)
- **Ingestion (OCR/Parsing)** : Docling + SentenceTransformers (GPU recommand√©)
- **LLM** : Llama.cpp (Qwen 30B - ou autre mod√®le GGUF)

## üìã Pr√©requis Rapides
- **GPU NVIDIA** (Recommand√© avec `nvidia-container-toolkit`)
- **RAM** : 24 Go minimum
- **Docker & Docker Compose**
- **Python 3.10+**

## ‚ö° Quick Start

Pour une installation d√©taill√©e, voir le [Manuel d'Utilisation](./MANUEL_UTILISATION.md).

1. **Pr√©parer les donn√©es** :
   Placez vos documents dans le dossier `wiki/` (ex: `wiki/niveau1-usagers/`).

2. **Lancer le LLM** :
   ```bash
   ./bin/llama-server --model <votre_modele.gguf> --port 8080 ...
   ```

3. **D√©marrer l'infrastructure** :
   ```bash
   docker-compose up -d
   ```

4. **Ing√©rer les documents** :
   ```bash
   sudo docker exec -it rag_api_usagers python ingest_with_nvidia.py
   ```

5. **Acc√©der √† l'interface** :
   Rendez-vous sur `http://localhost:5173`.

## üìö Documentation
- [Manuel d'Utilisation](./MANUEL_UTILISATION.md) : Guide complet d'installation et de d√©pannage.
- [API Docs](http://localhost:8000/docs) : Documentation Swagger de l'API.

## üë§ Auteur
Projet d√©velopp√© pour RAG-2i.
